       	       	     +-------------------------+
		     |		CS 140	       |
		     | PROJECT 4: FILE SYSTEMS |
		     |	   DESIGN DOCUMENT     |
		     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Xiaojiang Guo <gxj@stanford.edu>
Chunyan Wang  <chunyan@stanford.edu>
Yinfeng Qin   <yinfeng@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* On-disk inode. Must be exactly BLOCK_SECTOR_SIZE bytes long. */
struct inode_disk
  {
    off_t length;                       /* File size in bytes. */   
    off_t end;                          /* End of the portion of file
                                           actually written in bytes. */
    bool isdir;				/* Directory indicator */
    unsigned magic;                     /* Magic number. */

    block_sector_t blocks[NUM_DBLOCK + 2];
                                        /* Direct blocks and 2 more for 
                                           single, double indirect blocks */
    uint32_t unused[122 - NUM_DBLOCK];  /* Not used. */
  };

/* In-memory inode. */
struct inode 
  {
    /* ... */

    struct lock expand_lock;            /* Lock to prevent race conditions
                                           in inode expansion */

    /* We also removed the inode_disk member variable, and rely only on
       disk sector number to find correponding on-disk inode */

    /* ... */
  };


>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

We changed the structure of on-disk inode, which now looks like

     ----------
    |    0     |
     ----------      ----------
    |    1     |    |    0     |
     ----------      ----------
    |    :     |    |    1     |
    |    :     |     ----------               ----------
     ----------     |    :     |             |    0     |
    |    122   |  / |    :     |              ----------
     ----------  //  ----------          === |    :     |
    |          | /  |    127   |       //    |    :     |
    | indirect |     ----------       //      ----------
    |  block   |                     //      |    127   |
    |          |        ----------  //        ----------
     ----------        |    0     | /
    |          |        ----------    :
    |  doubly  |       |    1     |
    | indirect | =====  ----------    :
    |  block   |       |    :     |           ----------
    |          |       |    :     |   :      |    0     |
     ----------         ----------            ----------
                       |    127   | ======== |    :     |
                        ----------           |    :     |
                         indirect             ----------
                          blocks             |    127   |
                                              ----------

Now each inode_disk strucutre can hold a file with maximum size of
    512 * (122 + 128 + 128 * 128) = 8,516,608 Bytes ~ 8.12MB


---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

We add a lock in inode structure preventing simultaneous expansion of the
same file. When multiple process performs an expansion on a same file (also
on the same inode structure), only one of them can proceed with the acquired
lock. And following expansions would always check the actually file size to
determine if further expasion is needed.


>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

The majority of the logic determining whether or not an expasion is needed
is done by byte_to_sector() and the expand_inode().

When A and B both reach the end-of-file, A would compare the desired read
position with the actual end-of-file, which may have been updated by a 
write past previous end-of-file by B.

In our code, the length of file is updated only after all allocation and
actual writing of content has finished by B. This way, either A reaches
the previous end-of-file before B updates the file length, and A would
determine there is nothing to read, and return with no content read;
or A may reach the previous end-of-file after B updates the file length,
and by this time B have already allocated and written all contents
and properly updated end-of-file, so A can safely read the newly written
contents. In both cases, A will either read nothing or read what B writes.


>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

The main synchronization happens at buffer cache level. We implemented
shared lock for each block in buffer cache, allowing multiple process to
read together, while providing exclusive right to write on each block in
buffer cache. However, we do not force an exclusive use of the entire file
or inode strucutre, thus allowing multiple processes to access a file at
the same time.


---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

Our inode strucutre is a multilevel index. We choose the combination of
122 direct data blocks, together with 1 indirect block pointing to another
128 data blocks, and 1 doubly indirect block pointing to yet another
128 * 128 data blocks. Here the 122 is the maximum possible number of
direct blocks that a 512-byte sector can contain after all other relevant
information.

This choice of combination works well in real-life situations where most
files in a file system are small, while at the same time there are some
really large files. With our design, for most small files no larger than
122 blocks in size, we can find their sector and perform I/O to disk data
with two disk operations. When the files are really large, the system would
have perform I/O to disk with three or four operations, but this is the
rare case and the overall expected performance is still good.


			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* On-disk inode. Must be exactly BLOCK_SECTOR_SIZE bytes long. */
struct inode_disk
  {
    /* ... */

    bool isdir;				/* Directory indicator */

    /* ... */
  };

/* In-memory inode. */
struct inode 
  {
    /* ... */

    struct lock dir_lock;               /* Lock to prevent race conditions
                                           in directory operations */

    /* We also removed the inode_disk member variable, and rely only on
       disk sector number to find correponding on-disk inode */

    /* ... */
  };

struct thread
  {
    /* ... */

    struct dir *current_dir;            /* Current directory of this process */

    /* ... */
  };


---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

To traverse a user-specified path, we use the first character of given path
to decide whether it is a relative or absolute path. An absolute path starts
with "/", and the directory is opened starting from root directory; while for
relative paths, the process's current directory is used as the starting
point.

After choosing the starting point, we scan through the user-specified path.
At each level we find and open the newly parsed path, and keep going until
we run to the end, and arrive at the user-specified path.


---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

To prevent race condition on directory entries, we implemented a lock in
each inode strucutre for directory access. Since the directory's inode is
the only thing differentiating it from other directories, a lock inside the
inode strucutre could help us prevent multiple accesses to the same directory
but allow different directories to be opened or modified concurrently. 

For instance, to avoid two simultaneous attempts to remove a single file, the
working directory's lock inside its inode is acquired at the beginning of the
remove process. By doing so, other remove calls working on a different 
directory could happen at the same time, while operations on one directory is 
waiting for one another. The directory's lock is released until the end of 
remove process, so that remove operation in the same directory could not run
simultaneously. Each system call for modifying or creating a new file in the
same directory is only allowed to be done sequentially on system call level.
Though we do not take advantage of full parallelism, our implementation avoids
dangerous interleaves in the creation and deletion of directory entries.


>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

In our design, we disallow a directory to be removed right away if it is
opened or being used by any process other than the one performing "delete".
We just mark this directory as "to be removed", and only at the last "close"
operation on the directory do we check the "to be removed" status, and
perform the actual removal as indicated.

This is because if we allow the directory to be deleted when other processses 
still working on it, each time when a process needs to perform tasks on its 
current directory, it has to check whether its current directory is still
valid, since its current directory could be deleted by other processes without
any notification. This would cause a lot of waste in checking. It would also 
require extra overhead in synchroniztion if deletion is allowed, since a 
deletion could happen at any point when a process is opening or working on the
directory. Efforts have to be made to prevent race conditions in this case.
We believe that overall performance would be hampered if such deletion is
allowed, and this is why we disallow such deletion. 

To prevent the deletion of the opened directory, we added a function to check
if the directory is opened by other processes or not. As the directory's 
inode is always opened by the "delete" process for deletion, the inode's open
count would be larger than 2 if any other processes is working on it. Whenever
a deletion is executed, the open count is checked to decide whether the 
directory satifies deletion requirements. The directory's inode open count 
could serve as an effective judgement only if processes' working directory
actually opens the directory when working or openning it. This is why we 
implement a directory struct pointer in each process pointing to the working
directory. If a direcoty does not satisfy deletion requirments, the deletion 
would be aborted.


---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

We choose to represent the current directory of a process by using a directory
pointer to point at current directory. Each time a "chdir" system call is 
called, the previous directory is closed and the directory pointer is pointed
to the new directory. A new process's current directory is inherited from
its parent process, by reopening the parent process's directory. Another way
to record current directory is to use a string pointer to record the absolute
path of the process's current directory. One major disadvantage is that
every time opening a new file requires traversing the entire absolute path. 

Our method saves time and space especially when the process's current
directory is deep in the directory tree. By pointing to the current directory
struct, opening or creating a new file in current directory does not need to
open and search all the way from root. 

Another advantage of our method is that it makes it easier to determine if a
directory is being used by other processes. Whenever a directory is pointed
by a process, a dir_open call would be made, followed by an inode_open of the
directory's inode. This way it is easier to know if other processes are
pointing to the directory, which helps in the deletion procedure.


			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Shared lock that is used to protect cache read and write,
   Multiple readers or at most one writer are allowed */
struct shared_lock
{
  int i;
  struct lock lock;
  struct condition cond;
};

/* Cache metadata */
struct cache_block
{
  block_sector_t sector_no;	 	/* Sector number that is cached */
  bool dirty;				/* Dirty flag. If dirty when evicting,
                                   	   write the data back to disk */
  bool present;				/* Present bit */
  unsigned int time_stamp;		/* Time stamp for evicting algorithm */
  struct shared_lock shared_lock;       /* Shared lock for synchronization */
  void *data;				/* Cache data */
};

/* All 64 buffer cache blocks */
struct cache_block cache_block[64];

/* A lock on the eviction process */
struct lock evict_lock;

/* Flag indicating whether cache is set up */
bool cache_initialized = false;

struct read_struct			/* Struct used for read_ahead list */
  {
    struct list_elem elem;              /* List element for read_ahead_list */
    unsigned int sector;		/* Sector number */
  };

struct list read_ahead_list; 		/* Store all the read ahead request */
struct lock read_ahead_lock;		/* Lock to protect read_ahead_list */


---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

To choose a cache block to evict, we circle through cache blocks to find a
victim. A block could be chosen as victim only if it is not being currently
worked on. To ensure fairness (every block has equal opportunity to be
evicted), we add a time stamp to record the life span for each block. When
a cache block is newly used, an intial time stamp is initialized and decreased
in later circling search. To select a victim, we would search through all 
the cache blocks to find the one with minimal time stamp, namely the one
existing in the cache for the longest time. By doing so, not only are race
conditions avoided, but fairness is also ensured during the eviction process.


>> C3: Describe your implementation of write-behind.

When the system wants to write some data to disk, it begins by only copying
the data into buffer cache, and it is later that a background thread
periodically calls cache_flush() to actually write the data from buffer cache
to disk.

The background thread alternates between the two states all the time to
flush the cache: it would periodically sleep for a certain amount of time,
and then wake up and flush the cache.



>> C4: Describe your implementation of read-ahead.

The reading data from a file is done in inode_read_at () function. So each 
time before a cache read request is issued, we use byte_to_sector () function
to calculate sector number of the next block to read. If the number differs
from the current one, then read-ahead needs to be performed. 

In doing this, we set up a read_ahead_list to store all read-ahead requests, 
and when a read-ahead is needed, we push that request in the list. Furthermore,
we set up a new thread "read_ahead" in the background that executing a loop,
processing all the request in read-ahead list and then yields immediately.
With proper synchronization of the buffer cache described below in C5 and C6,
read-ahead can be implement correctly without race conditions.


---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

For synchronization in buffer cache, we use two different types of locks.
One is a simple lock for eviction algorithm, which is used to prevent two
threads to look for victim to evict simultaneously. Another kind of lock
is shared lock for each of the buffer cache. It adopts the reader-writer
shared lock model, and has exclusive mode and shared mode. Shared mode is 
designed to allow multiple readers reading data from data cache to memory
or to disk at the same time. For example, cache_flush () and reading data
from buffer cache to memory use shared lock in shared mode. While exclusive
mode is used when buffer cache data is modified by writer. For example, in 
cache_read, in case of cache miss, the data is fetched from the disk to the 
cache, or when data is written from memory to buffer cache in cache_write,
shared lock is used in exclusive mode. Note that different buffer cache 
blocks use independent lock so that concurrent operations on different cache 
blocks are allowed.

When a cache_read or cache_write request is issued, we use the eviction lock
to protect the eviction process. After a cache hit or miss(and a victim block
is selected in this case), we acquire the shared lock for that cache block in 
exclusive mode. This is because no matter cache_read or cache_write, the first 
possible operation, i.e. reading data from disk to buffer cache for cache_read,
and writing data from memory to buffer cache for cache_write, does always 
modify the content of buffer cache and thus has to be protected by shared lock
in exclusive mode. But for cache read, after a possible fetch from disk, we 
change the mode of the shared lock from exclusive mode to shared mode to allow
multiple readers, because next operation for cache_read is to read data from 
cache to memory. 

Additionally, we disable interrupt in switching between eviction lock cache
lock, and between cache lock in exclusive mode and cache lock in shared mode,
in doing this, we prevent racing condition.

Back to the question, when a process is writing or reading a buffer cache 
block, it acquires the lock, so other process wanting to evict the buffer 
cache must acquire a shared mode to write the cache block data to disk.
So for writing buffer cache, this cannot be done because writing buffer
cache needs the lock in exclusive mode; but for reading the buffer cache, 
the buffer cache can be actually being "evicted". By "evicted" I mean 
writing data from buffer to disk, without actually modifying the data, 
the unchanged data is guaranteed by the shared mode cache lock held by the
thread reading buffer cache. In this case, you can see that our design makes
room for maximum degree of parallelism while ensuring correctness.


>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

This is very similar to C5. In evicting a block from the cache, the threads
holds a shared mode lock for that cache. So accessing the block by other 
threads is permitted if in a shared mode, and denied if in a exclusive mode.

For example, a thread reading cache data from cache to memory after a cache
hit is allowed, and a thread writing data to that cache or binging data 
from disk to that cache is not allowed. You may want to know what happened 
if the cache is evicted before another thread finishes reading data from 
cache to memory after a cache hit. Actually this is impossible to happen.
Evicting a cache block consists of two steps, firstly writing the cache data 
to disk and then fetching new data to the cache. During the first step,
another thread reading data from that cache is allowed; and before this 
thread finishes reading, the original thread cannot enter the second step 
of eviction, because the lock in the exclusive mode is required. As a result, 
it waits until the second thread finishes its work and releases the lock
in the shared mode. If now no other thread is reading the cache, the original
thread can acquire the lock in exclusive mode and finishes evicting the cache
block. So in this case, you can see that our design makes room for maximum 
degree of parallelism while ensuring correctness.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

One typical file workload that is likely to benefit from buffer caching
is in our file growth procedure. This workload involves many read, update
and write back to disk on the indirect and doubly indirect inodes when there 
is a huge growth. In this case, by caching the indirect and doubly indirect
inodes, the performance would be significantly improved.

A file workload that involved sequential read and computing would benefit
from read-ahead. For example, when loading a large file spanning many block
sectors, with read-ahead, we may already have the following block in cache
after loading the first block and performed some other operations. This saves
many disk operations for the following block reads.

A file workload that involves interleaved computing and writing would benefit
from write-behind. Without write-behind, the process would have to wait for
the disk write to complete before continuing the computing, thus wasting many
CPU cycles waiting for the disk write. With write-behind, the process can
write the data to buffer cache very quickly and proceed with the computing,
and the actual writing to disk would continue in background when the CPU is
relatively free. The overall performance under this kind of workload would
benefit from write-behind.


			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?
